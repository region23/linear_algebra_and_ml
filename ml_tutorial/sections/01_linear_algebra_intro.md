# Основы линейной алгебры для машинного обучения

## Введение

Здравствуйте! Если вы хотите погрузиться в мир машинного обучения и искусственного интеллекта, то линейная алгебра — это один из фундаментальных инструментов, который вам необходимо освоить. Не беспокойтесь, если математика не была вашей сильной стороной в школе или университете. Мы постараемся объяснить все концепции максимально просто и наглядно, опираясь на ваш опыт разработки на Go и базовые знания Python.

Линейная алгебра — это раздел математики, который изучает векторы, матрицы и линейные преобразования. В контексте машинного обучения и особенно языковых моделей (LLM), линейная алгебра играет ключевую роль, поскольку:

1. Данные в машинном обучении представляются в виде векторов и матриц
2. Большинство алгоритмов машинного обучения используют операции линейной алгебры
3. Модели глубокого обучения, включая трансформеры (используемые в LLM), основаны на матричных вычислениях

В этом разделе мы рассмотрим основные концепции линейной алгебры, которые необходимы для понимания работы моделей машинного обучения, и покажем, как эти концепции применяются на практике.

## Векторы: основа всего

### Что такое вектор?

В программировании вы привыкли работать с массивами и слайсами. Вектор в линейной алгебре — это по сути то же самое: упорядоченный набор чисел. Например, вектор из трех элементов можно записать как:

```
v = [3, 1, 4]
```

В машинном обучении векторы используются повсеместно:
- Признаки объекта (например, рост, вес, возраст человека)
- Веса в нейронной сети
- Представления слов в NLP (word embeddings)
- Градиенты функций потерь

### Операции с векторами

Основные операции с векторами, которые вам нужно знать:

1. **Сложение векторов**: выполняется поэлементно
   ```
   [a, b, c] + [d, e, f] = [a+d, b+e, c+f]
   ```

2. **Умножение вектора на скаляр**: каждый элемент умножается на число
   ```
   k * [a, b, c] = [k*a, k*b, k*c]
   ```

3. **Скалярное произведение**: сумма произведений соответствующих элементов
   ```
   [a, b, c] · [d, e, f] = a*d + b*e + c*f
   ```

4. **Норма вектора**: мера "длины" вектора
   ```
   ||v|| = √(v₁² + v₂² + ... + vₙ²)
   ```

### Пример на Python

```python
import numpy as np

# Создание векторов
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

# Сложение векторов
sum_v = v1 + v2  # [5, 7, 9]

# Умножение на скаляр
scaled_v = 2 * v1  # [2, 4, 6]

# Скалярное произведение
dot_product = np.dot(v1, v2)  # 1*4 + 2*5 + 3*6 = 32

# Норма вектора
norm_v1 = np.linalg.norm(v1)  # √(1² + 2² + 3²) ≈ 3.74
```

## Матрицы: двумерные массивы данных

### Что такое матрица?

Матрица — это прямоугольная таблица чисел, организованная в строки и столбцы. Если вы работали с двумерными массивами в Go, то уже знакомы с этой концепцией.

Пример матрицы размером 2×3 (2 строки, 3 столбца):
```
A = [
    [1, 2, 3],
    [4, 5, 6]
]
```

В машинном обучении матрицы используются для:
- Представления наборов данных (каждая строка — объект, каждый столбец — признак)
- Хранения весов в слоях нейронных сетей
- Представления преобразований
- Вычисления ковариаций между признаками

### Основные операции с матрицами

1. **Сложение матриц**: выполняется поэлементно (матрицы должны иметь одинаковые размеры)
   ```
   [a, b] + [e, f] = [a+e, b+f]
   [c, d]   [g, h]   [c+g, d+h]
   ```

2. **Умножение матрицы на скаляр**: каждый элемент умножается на число
   ```
   k * [a, b] = [k*a, k*b]
       [c, d]   [k*c, k*d]
   ```

3. **Умножение матриц**: строки первой матрицы умножаются на столбцы второй
   ```
   [a, b] × [e, f] = [a*e + b*g, a*f + b*h]
   [c, d]   [g, h]   [c*e + d*g, c*f + d*h]
   ```

4. **Транспонирование**: строки становятся столбцами, а столбцы — строками
   ```
   [a, b]ᵀ = [a, c]
   [c, d]    [b, d]
   ```

### Пример на Python

```python
import numpy as np

# Создание матриц
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Сложение матриц
sum_matrix = A + B
# [[6, 8],
#  [10, 12]]

# Умножение на скаляр
scaled_matrix = 2 * A
# [[2, 4],
#  [6, 8]]

# Умножение матриц
product_matrix = np.dot(A, B)
# [[19, 22],
#  [43, 50]]

# Транспонирование
A_transposed = A.T
# [[1, 3],
#  [2, 4]]
```

## Системы линейных уравнений

Системы линейных уравнений — это набор уравнений вида:
```
a₁₁x₁ + a₁₂x₂ + ... + a₁ₙxₙ = b₁
a₂₁x₁ + a₂₂x₂ + ... + a₂ₙxₙ = b₂
...
aₘ₁x₁ + aₘ₂x₂ + ... + aₘₙxₙ = bₘ
```

Такую систему можно записать в матричной форме:
```
Ax = b
```
где A — матрица коэффициентов, x — вектор неизвестных, b — вектор правых частей.

### Решение систем линейных уравнений

В машинном обучении часто требуется решать системы линейных уравнений. Например, в линейной регрессии мы ищем веса, которые минимизируют ошибку предсказания.

Решение системы Ax = b можно найти как:
```
x = A⁻¹b
```
где A⁻¹ — обратная матрица к A.

### Пример на Python

```python
import numpy as np

# Система уравнений:
# 2x + y = 5
# 3x + 2y = 8

# Матрица коэффициентов
A = np.array([[2, 1], [3, 2]])

# Вектор правых частей
b = np.array([5, 8])

# Решение системы
x = np.linalg.solve(A, b)
# x ≈ [2, 1]

# Проверка
np.dot(A, x)  # [5, 8]
```

## Линейные преобразования и собственные значения

### Линейные преобразования

Линейное преобразование — это функция, которая сохраняет операции сложения и умножения на скаляр. В машинном обучении линейные преобразования используются повсеместно, например, в слоях нейронных сетей.

Любое линейное преобразование можно представить в виде умножения на матрицу:
```
T(v) = Av
```

Примеры линейных преобразований:
- Поворот
- Масштабирование
- Отражение
- Проекция

### Собственные значения и собственные векторы

Собственный вектор матрицы A — это ненулевой вектор v, который при умножении на A меняет только свою длину, но не направление:
```
Av = λv
```
где λ — собственное значение.

Собственные значения и векторы используются в:
- Методе главных компонент (PCA) для снижения размерности
- Спектральной кластеризации
- Анализе устойчивости систем

### Пример на Python

```python
import numpy as np

# Создание матрицы
A = np.array([[4, 2], [1, 3]])

# Нахождение собственных значений и векторов
eigenvalues, eigenvectors = np.linalg.eig(A)

print("Собственные значения:", eigenvalues)
# Собственные значения: [5. 2.]

print("Собственные векторы:")
print(eigenvectors)
# Собственные векторы:
# [[0.89442719 0.4472136 ]
#  [0.4472136  0.89442719]]

# Проверка для первого собственного вектора
v1 = eigenvectors[:, 0]
lambda1 = eigenvalues[0]
np.dot(A, v1) - lambda1 * v1  # ≈ [0, 0]
```

## Сингулярное разложение (SVD)

Сингулярное разложение (Singular Value Decomposition, SVD) — это один из самых мощных инструментов линейной алгебры, который используется во многих алгоритмах машинного обучения.

SVD разлагает матрицу A размером m×n на произведение трех матриц:
```
A = UΣVᵀ
```
где:
- U — ортогональная матрица размером m×m
- Σ — диагональная матрица размером m×n с неотрицательными действительными числами на диагонали
- Vᵀ — транспонированная ортогональная матрица размером n×n

### Применение SVD в машинном обучении

1. **Снижение размерности**: можно аппроксимировать исходную матрицу, оставив только k наибольших сингулярных значений
2. **Метод главных компонент (PCA)**: тесно связан с SVD
3. **Рекомендательные системы**: используется в коллаборативной фильтрации
4. **Обработка изображений**: сжатие, шумоподавление
5. **Решение систем линейных уравнений**: особенно для плохо обусловленных систем

### Пример на Python

```python
import numpy as np

# Создание матрицы
A = np.array([[1, 2], [3, 4], [5, 6]])

# Выполнение SVD
U, S, Vt = np.linalg.svd(A)

print("Матрица U:")
print(U)

print("Сингулярные значения:")
print(S)

print("Матрица V^T:")
print(Vt)

# Восстановление исходной матрицы
# Создаем диагональную матрицу Sigma
Sigma = np.zeros((A.shape[0], A.shape[1]))
Sigma[:A.shape[1], :A.shape[1]] = np.diag(S)

# A = U * Sigma * V^T
A_reconstructed = U.dot(Sigma.dot(Vt))
print("Восстановленная матрица:")
print(A_reconstructed)  # Должна быть близка к исходной A
```

## Практические примеры применения линейной алгебры в ML

### 1. Линейная регрессия

Линейная регрессия — это простейший алгоритм машинного обучения, который моделирует зависимость между входными признаками и целевой переменной как линейную функцию.

Если у нас есть матрица признаков X и вектор целевых значений y, то веса линейной модели можно найти по формуле:
```
w = (XᵀX)⁻¹Xᵀy
```

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# Создание данных
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])  # Признаки
y = np.array([6, 8, 9, 11])  # Целевые значения

# Обучение модели
model = LinearRegression().fit(X, y)

# Получение коэффициентов
print("Коэффициенты:", model.coef_)  # [1.5 2. ]
print("Свободный член:", model.intercept_)  # 2.5

# Предсказание
print("Предсказание:", model.predict(np.array([[3, 5]])))  # [15.5]
```

### 2. Метод главных компонент (PCA)

PCA — это метод снижения размерности, который проецирует данные на подпространство с максимальной дисперсией.

```python
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Создание данных
np.random.seed(42)
X = np.random.randn(100, 2)
X = X.dot(np.array([[2, 1], [1, 3]]))  # Создаем корреляцию между признаками

# Применение PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Визуализация
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1])
plt.title('Исходные данные')

plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.title('Данные после PCA')

plt.tight_layout()
plt.savefig('/home/ubuntu/ml_tutorial/sections/pca_example.png')

print("Объясненная дисперсия:", pca.explained_variance_ratio_)
print("Компоненты:", pca.components_)
```

### 3. Метод опорных векторов (SVM)

SVM — это алгоритм классификации, который находит гиперплоскость, максимально разделяющую классы.

```python
import numpy as np
from sklearn import svm
import matplotlib.pyplot as plt

# Создание данных
np.random.seed(42)
X = np.random.randn(20, 2)
y = np.array([1] * 10 + [-1] * 10)
X[y == -1] += np.array([2, 2])  # Сдвигаем отрицательные примеры

# Обучение SVM
clf = svm.SVC(kernel='linear')
clf.fit(X, y)

# Визуализация
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)

# Построение разделяющей гиперплоскости
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Создание сетки для оценки модели
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

# Построение границы решения и полей
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])

# Выделение опорных векторов
ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k')

plt.title('SVM с линейным ядром')
plt.savefig('/home/ubuntu/ml_tutorial/sections/svm_example.png')
```

## Заключение

Линейная алгебра — это фундаментальный инструмент в машинном обучении. Понимание векторов, матриц и линейных преобразований позволяет глубже понять, как работают алгоритмы машинного обучения, включая нейронные сети и языковые модели.

В следующих разделах мы будем опираться на эти знания, чтобы изучить основы машинного обучения, нейронные сети и, в конечном итоге, языковые модели (LLM).

Не беспокойтесь, если некоторые концепции кажутся сложными — мы будем возвращаться к ним по мере необходимости и рассматривать их в контексте конкретных задач машинного обучения.