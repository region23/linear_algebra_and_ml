# Языковые модели (LLM)

## Введение в обработку естественного языка (NLP)

Обработка естественного языка (Natural Language Processing, NLP) — это область искусственного интеллекта, которая занимается взаимодействием между компьютерами и человеческим языком. Цель NLP — научить компьютеры понимать, интерпретировать и генерировать человеческий язык в полезной форме.

NLP объединяет компьютерную науку, искусственный интеллект и лингвистику для создания систем, способных обрабатывать и анализировать большие объемы языковых данных. Эта область имеет множество практических применений, включая:

- Машинный перевод (например, Google Translate)
- Распознавание и синтез речи (например, Siri, Алиса)
- Анализ тональности текста (определение эмоциональной окраски)
- Извлечение информации из текста
- Автоматическое реферирование текстов
- Чат-боты и виртуальные ассистенты
- Системы вопросов и ответов

Традиционные подходы к NLP включали правила, созданные лингвистами, и статистические методы. Однако в последние годы глубокое обучение произвело революцию в этой области, особенно с появлением больших языковых моделей (Large Language Models, LLM).

## Векторные представления слов (Word Embeddings)

Одной из ключевых проблем в NLP является представление слов в форме, понятной для компьютера. Компьютеры не могут напрямую работать с текстом — им нужны числа. Векторные представления слов (или эмбеддинги) решают эту проблему, преобразуя слова в векторы чисел в многомерном пространстве.

### Что такое эмбеддинги?

Эмбеддинги — это способ представления текста в виде векторов в многомерном пространстве (обычно от 100 до 1536 измерений). В этом пространстве слова с похожим значением или контекстом располагаются близко друг к другу. Например, слова "кошка" и "собака" будут ближе друг к другу, чем "кошка" и "автомобиль".

Важно понимать, что эмбеддинги не просто нумеруют слова (1, 2, 3...), а создают содержательное представление, где расположение в пространстве отражает семантические отношения между словами.

### Типы эмбеддингов

1. **Word2Vec** — один из первых популярных методов создания эмбеддингов, разработанный Google в 2013 году. Существует в двух вариантах:
   - CBOW (Continuous Bag of Words) — предсказывает целевое слово на основе окружающих слов
   - Skip-gram — предсказывает окружающие слова на основе целевого слова

2. **GloVe (Global Vectors)** — метод, разработанный в Стэнфордском университете, который объединяет преимущества матричной факторизации и локальных контекстных окон.

3. **FastText** — расширение Word2Vec от Facebook, которое учитывает морфологию слов, разбивая их на n-граммы (подпоследовательности символов).

4. **Contextual Embeddings** — современные эмбеддинги, которые учитывают контекст слова в предложении:
   - BERT embeddings
   - GPT embeddings
   - ELMo (Embeddings from Language Models)

### Пример создания эмбеддингов с помощью Word2Vec

```python
import gensim.downloader as api
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Загрузка предобученной модели Word2Vec
word2vec_model = api.load('word2vec-google-news-300')

# Выбор нескольких слов для визуализации
words = ['кошка', 'собака', 'мышь', 'компьютер', 'клавиатура', 'монитор']
word_vectors = [word2vec_model[word] for word in words if word in word2vec_model]
words = [word for word in words if word in word2vec_model]

# Уменьшение размерности до 2D для визуализации
pca = PCA(n_components=2)
result = pca.fit_transform(word_vectors)

# Визуализация
plt.figure(figsize=(10, 8))
plt.scatter(result[:, 0], result[:, 1], c='blue')

for i, word in enumerate(words):
    plt.annotate(word, xy=(result[i, 0], result[i, 1]))

plt.title('Визуализация векторных представлений слов')
plt.savefig('/home/ubuntu/ml_tutorial/sections/word_embeddings_visualization.png')
```

## Архитектура современных языковых моделей

Большие языковые модели (LLM) — это модели машинного обучения, обученные на огромных объемах текстовых данных для понимания и генерации текста, похожего на человеческий. Термин "большие" относится к количеству параметров модели, которое может достигать сотен миллиардов.

### Эволюция языковых моделей

1. **Статистические языковые модели** — ранние модели, основанные на n-граммах и статистических методах.

2. **Рекуррентные нейронные сети (RNN)** — сети, способные обрабатывать последовательные данные, сохраняя информацию о предыдущих входах.

3. **LSTM (Long Short-Term Memory)** и **GRU (Gated Recurrent Unit)** — улучшенные версии RNN, решающие проблему исчезающего градиента и способные запоминать долгосрочные зависимости.

4. **Трансформеры** — современная архитектура, представленная в статье "Attention is All You Need" (2017), которая произвела революцию в NLP благодаря механизму внимания.

5. **Предобученные языковые модели** — модели, обученные на огромных корпусах текста без учителя, а затем дообученные для конкретных задач:
   - BERT, RoBERTa, T5 (энкодер-декодер)
   - GPT, LLaMA, Claude (декодер)

### Ключевые компоненты архитектуры трансформера

Архитектура трансформера состоит из двух основных компонентов:

1. **Энкодер (Encoder)** — обрабатывает входную последовательность и создает её представление.
2. **Декодер (Decoder)** — генерирует выходную последовательность на основе представления, созданного энкодером.

Некоторые модели используют только энкодер (BERT), только декодер (GPT) или полную архитектуру энкодер-декодер (T5, BART).

Основные элементы трансформера:

- **Механизм внимания (Attention Mechanism)** — позволяет модели фокусироваться на различных частях входной последовательности при генерации каждого элемента выходной последовательности.
- **Многоголовое внимание (Multi-Head Attention)** — несколько механизмов внимания, работающих параллельно, что позволяет модели фокусироваться на разных аспектах входных данных.
- **Позиционное кодирование (Positional Encoding)** — добавляет информацию о позиции слова в последовательности, так как сам механизм внимания не учитывает порядок слов.
- **Нормализация слоя (Layer Normalization)** — стабилизирует обучение, нормализуя активации в каждом слое.
- **Остаточные соединения (Residual Connections)** — помогают бороться с проблемой исчезающего градиента, позволяя информации напрямую проходить через слои.
- **Полносвязные слои (Feed-Forward Networks)** — обрабатывают выходы механизма внимания.

## Трансформеры в языковых моделях (BERT, GPT, LLaMA)

### BERT (Bidirectional Encoder Representations from Transformers)

BERT, разработанный Google в 2018 году, использует только энкодерную часть трансформера. Ключевые особенности:

- **Двунаправленность** — учитывает контекст слова с обеих сторон (слева и справа), что позволяет лучше понимать значение слов в контексте.
- **Предобучение** — обучается на двух задачах:
  1. Masked Language Modeling (MLM) — предсказание маскированных слов в предложении
  2. Next Sentence Prediction (NSP) — определение, следует ли одно предложение за другим

BERT хорошо подходит для задач понимания языка, таких как классификация текста, извлечение информации, ответы на вопросы, но менее эффективен для генерации текста.

Варианты BERT:
- BERT-base (110M параметров)
- BERT-large (340M параметров)
- RoBERTa — оптимизированная версия BERT от Facebook
- DistilBERT — облегченная версия BERT

### GPT (Generative Pre-trained Transformer)

GPT, разработанный OpenAI, использует только декодерную часть трансформера. Ключевые особенности:

- **Автореграссивность** — генерирует текст последовательно, слово за словом, основываясь на предыдущих словах.
- **Однонаправленность** — учитывает только предыдущий контекст (слева направо), что делает его идеальным для генерации текста.
- **Предобучение и дообучение** — сначала обучается на огромном корпусе текста без учителя, затем дообучается на конкретных задачах.

Эволюция GPT:
- GPT-1 (2018) — 117M параметров
- GPT-2 (2019) — до 1.5B параметров
- GPT-3 (2020) — 175B параметров
- GPT-3.5 (2022) — улучшенная версия GPT-3, используемая в ChatGPT
- GPT-4 (2023) — мультимодальная модель с улучшенными возможностями

GPT отлично подходит для генерации текста, перевода, написания контента, но может "галлюцинировать" (генерировать правдоподобную, но неверную информацию).

### LLaMA (Large Language Model Meta AI)

LLaMA — семейство моделей, разработанных Meta (Facebook) в 2023 году. Ключевые особенности:

- **Эффективность** — достигает производительности, сравнимой с GPT-3, но с меньшим количеством параметров.
- **Открытость** — модели доступны исследователям, что способствует развитию сообщества.
- **Масштабируемость** — представлены в различных размерах (от 7B до 65B параметров).

Варианты LLaMA:
- LLaMA 1 — первая версия, выпущенная в феврале 2023
- LLaMA 2 — улучшенная версия с лучшей производительностью и безопасностью
- Llama 2-Chat — версия, оптимизированная для диалогов

LLaMA стала основой для многих производных моделей, созданных сообществом, таких как Alpaca, Vicuna, Koala и других.

### Сравнение архитектур

| Модель | Тип | Направленность | Основное применение | Размер (параметры) |
|--------|-----|----------------|---------------------|-------------------|
| BERT   | Энкодер | Двунаправленная | Понимание языка | 110M-340M |
| GPT    | Декодер | Однонаправленная | Генерация текста | 117M-175B+ |
| LLaMA  | Декодер | Однонаправленная | Генерация текста | 7B-65B |
| T5     | Энкодер-декодер | Двунаправленная (энкодер) и однонаправленная (декодер) | Универсальные задачи | 220M-11B |

## Тонкая настройка (Fine-tuning) предобученных моделей

Тонкая настройка (fine-tuning) — это процесс адаптации предобученной языковой модели для конкретной задачи или домена путем дообучения на специализированном наборе данных. Этот подход позволяет использовать знания, полученные моделью во время предобучения на огромном корпусе текста, и адаптировать их для решения конкретных задач.

### Преимущества тонкой настройки

1. **Эффективность ресурсов** — требует значительно меньше вычислительных ресурсов и данных, чем обучение модели с нуля.
2. **Лучшая производительность** — предобученные модели уже имеют общее понимание языка, что дает хорошую отправную точку.
3. **Быстрая адаптация** — можно быстро адаптировать модель для новых задач или доменов.

### Методы тонкой настройки

1. **Полная тонкая настройка (Full Fine-tuning)** — обновление всех параметров модели. Требует больше вычислительных ресурсов, но может дать лучшие результаты.

2. **Адаптеры (Adapters)** — добавление небольших обучаемых модулей между слоями предобученной модели, оставляя основные параметры замороженными. Это экономит ресурсы и предотвращает катастрофическое забывание.

3. **Prompt-tuning** — обучение непрерывных подсказок (continuous prompts), которые добавляются к входным данным, при этом параметры модели остаются неизменными.

4. **LoRA (Low-Rank Adaptation)** — метод, который добавляет низкоранговые матрицы к весам предобученной модели, значительно сокращая количество обучаемых параметров.

5. **QLoRA** — комбинация квантизации и LoRA, позволяющая настраивать большие модели на ограниченных ресурсах.

### Процесс тонкой настройки

1. **Подготовка данных** — сбор и подготовка специализированного набора данных для конкретной задачи.
2. **Выбор базовой модели** — выбор предобученной модели, подходящей для задачи (BERT для понимания, GPT для генерации и т.д.).
3. **Настройка гиперпараметров** — выбор скорости обучения, размера батча, количества эпох и т.д.
4. **Обучение** — дообучение модели на специализированном наборе данных.
5. **Оценка** — оценка производительности модели на тестовом наборе данных.
6. **Итерация** — при необходимости корректировка гиперпараметров и повторное обучение.

### Пример тонкой настройки BERT для классификации текста

```python
import torch
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import numpy as np

# Подготовка данных (пример)
texts = ["Этот фильм был потрясающим!", "Ужасный сервис в этом ресторане.", ...]
labels = [1, 0, ...]  # 1 - положительный, 0 - отрицательный

# Разделение на обучающую и тестовую выборки
train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)

# Загрузка токенизатора и модели
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)

# Токенизация текстов
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')

# Создание датасетов
train_dataset = TensorDataset(
    train_encodings['input_ids'], 
    train_encodings['attention_mask'], 
    torch.tensor(train_labels)
)
test_dataset = TensorDataset(
    test_encodings['input_ids'], 
    test_encodings['attention_mask'], 
    torch.tensor(test_labels)
)

# Создание даталоадеров
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# Настройка оптимизатора
optimizer = AdamW(model.parameters(), lr=5e-5)

# Обучение модели
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

model.train()
for epoch in range(3):  # обычно достаточно 2-4 эпох
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Оценка модели
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=1)
        correct += (predictions == labels).sum().item()
        total += labels.size(0)

accuracy = correct / total
print(f"Точность: {accuracy:.4f}")

# Сохранение модели
model.save_pretrained('/home/ubuntu/ml_tutorial/models/bert_sentiment')
tokenizer.save_pretrained('/home/ubuntu/ml_tutorial/models/bert_sentiment')
```

## Практические примеры работы с LLM на Python

### Пример 1: Использование Hugging Face Transformers для генерации текста с GPT-2

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Загрузка предобученной модели и токенизатора
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Подготовка входного текста
input_text = "Искусственный интеллект в будущем сможет"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Генерация текста
output = model.generate(
    input_ids,
    max_length=100,
    num_return_sequences=3,
    temperature=0.8,
    top_k=50,
    top_p=0.95,
    do_sample=True,
    no_repeat_ngram_size=2,
    early_stopping=True
)

# Декодирование и вывод результатов
for i, sequence in enumerate(output):
    text = tokenizer.decode(sequence, skip_special_tokens=True)
    print(f"Сгенерированный текст {i+1}:\n{text}\n")
```

### Пример 2: Анализ тональности текста с помощью BERT

```python
from transformers import pipeline

# Создание пайплайна для анализа тональности
sentiment_analyzer = pipeline("sentiment-analysis", model="blanchefort/rubert-base-cased-sentiment")

# Тексты для анализа
texts = [
    "Этот фильм просто потрясающий, я в восторге!",
    "Сервис в ресторане был ужасным, больше туда не пойду.",
    "Книга оказалась неплохой, но я ожидал большего."
]

# Анализ тональности
results = sentiment_analyzer(texts)

# Вывод результатов
for text, result in zip(texts, results):
    print(f"Текст: {text}")
    print(f"Тональность: {result['label']}, Уверенность: {result['score']:.4f}\n")
```

### Пример 3: Суммаризация текста с помощью T5

```python
from t
(Content truncated due to size limit. Use line ranges to read in chunks)